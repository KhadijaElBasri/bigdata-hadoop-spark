# Big Data Text Processing with Hadoop & Spark

## Overview

This repository presents a **Big Data text processing project** focused on distributed word count analysis using **Hadoop MapReduce** and **Apache Spark**.

The goal of the project is to design and execute a **complete Big Data workflow**, from raw data preparation to distributed processing on **HDFS**, and to compare **MapReduce and Spark** in terms of performance, flexibility, and ease of development.

The dataset consists of large public-domain text files (books), processed on a Hadoop/Spark environment using **Python** and **PySpark**.



## Key Features

- Distributed word count using **Hadoop MapReduce (Streaming â€“ Python)**
- In-memory distributed processing using **Apache Spark (PySpark)**
- Data cleaning and preprocessing before ingestion
- Execution on **HDFS**
- Performance comparison between MapReduce and Spark
- Clear and reproducible execution workflow



## Technologies Used

- **Hadoop** (HDFS, YARN)
- **Hadoop MapReduce** (Streaming with Python)
- **Apache Spark / PySpark**
- **Python**
- Linux / WSL environment
- Distributed data processing concepts



## Repository Structure

```

bigdata-hadoop-spark/
â”‚
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ mapper.py                 # MapReduce mapper (Python)
â”œâ”€â”€ reducer.py                # MapReduce reducer (Python)
â”œâ”€â”€ wordcount_spark.py        # Spark word count script (PySpark)
â”‚
â”œâ”€â”€ data_raw/                 # Raw text data
â”‚   â”œâ”€â”€ les_miserables_en.txt
â”‚   â””â”€â”€ monte_cristo_en.txt
â”‚
â””â”€â”€ data_clean/               
    â”œâ”€â”€ books_clean.txt           # Cleaned data generated by clean_text.py
    â””â”€â”€ clean_text.py             # Data cleaning and consolidation script


````

> Output directories generated by Hadoop and Spark are not included to keep the repository lightweight.  
> They can be regenerated by following the steps below.


## How to Run the Project

### 1. Data Preparation

Place the raw text files inside `data_raw/`, then run:

```bash
python3 clean_text.py
````

This script generates a cleaned and consolidated file:

```
data_clean/books_clean.txt
```


### 2. Run Hadoop MapReduce

Create the input directory in HDFS and upload the cleaned file:

```bash
hdfs dfs -mkdir -p /user/<your_username>/bigdata/input
hdfs dfs -put data_clean/books_clean.txt /user/<your_username>/bigdata/input/
```

Launch the MapReduce job using Hadoop Streaming:

```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -input /user/<your_username>/bigdata/input/books_clean.txt \
  -output /user/<your_username>/bigdata/output_mapreduce \
  -mapper mapper.py \
  -reducer reducer.py
```

View the first results:

```bash
hdfs dfs -cat /user/<your_username>/bigdata/output_mapreduce/part-* | head -n 20
```


### 3. Run Apache Spark (PySpark)

Submit the Spark job:

```bash
spark-submit wordcount_spark.py
```

Results are stored in HDFS under:

```
/user/<your_username>/bigdata/output_spark
```

Display the first results:

```bash
hdfs dfs -cat /user/<your_username>/bigdata/output_spark/part-* | head -n 20
```


## Results & Insights

* Both MapReduce and Spark produce consistent word count results.
* **Apache Spark** is significantly faster (â‰ˆ2â€“3x) and easier to develop thanks to in-memory processing.
* **MapReduce** remains reliable for large batch processing but requires more boilerplate and execution time.

This project helped me strengthen my practical understanding of **distributed systems**, **Big Data processing**, **Hadoop**, and **Spark**, as well as their real-world trade-offs.



## About Me

**Khadija El Basri**
Second-year DUT student in Data Engineering
Ã‰cole SupÃ©rieure de Technologie â€“ El Kelaa des Sraghna

- ðŸ“§ Email: [khadijaelbasri.dev@gmail.com](mailto:khadijaelbasri.dev@gmail.com)
- ðŸ”— LinkedIn: [linkedin.com/in/khadija-el-basri](https://linkedin.com/in/khadija-el-basri)  
- ðŸ’» GitHub: [github.com/KhadijaElBasri](https://github.com/KhadijaElBasri)

